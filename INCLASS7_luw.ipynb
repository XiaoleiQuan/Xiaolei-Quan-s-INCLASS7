{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N11Ee3GJmywu",
        "outputId": "3e1023bf-7a6a-4341-a9c0-0078d5cdf63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install openai wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Q2A8TGhKm3i5"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9HEMJSX-3T"
      },
      "source": [
        "# 1.) Set up OpenAI and the enviornment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4zwwdkZDYDZN"
      },
      "outputs": [],
      "source": [
        "# DONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "8IiKS0snlpYP"
      },
      "outputs": [],
      "source": [
        "apikey = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "E0y9Y_Q6Nrok"
      },
      "outputs": [],
      "source": [
        "openai.api_key = apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "-vgWY7vMNrok"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(\n",
        "    api_key = openai.api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOXc5_BTm9HP"
      },
      "source": [
        "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "-v7OYamHlrEB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "298f626b-0cd4-4fe7-bc24-9f60a87af87c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['API_URL',\n",
              " 'BeautifulSoup',\n",
              " 'Decimal',\n",
              " 'DisambiguationError',\n",
              " 'HTTPTimeoutError',\n",
              " 'ODD_ERROR_MESSAGE',\n",
              " 'PageError',\n",
              " 'RATE_LIMIT',\n",
              " 'RATE_LIMIT_LAST_CALL',\n",
              " 'RATE_LIMIT_MIN_WAIT',\n",
              " 'RedirectError',\n",
              " 'USER_AGENT',\n",
              " 'WikipediaException',\n",
              " 'WikipediaPage',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'cache',\n",
              " 'datetime',\n",
              " 'debug',\n",
              " 'donate',\n",
              " 'exceptions',\n",
              " 'geosearch',\n",
              " 'languages',\n",
              " 'page',\n",
              " 'random',\n",
              " 're',\n",
              " 'requests',\n",
              " 'search',\n",
              " 'set_lang',\n",
              " 'set_rate_limiting',\n",
              " 'set_user_agent',\n",
              " 'stdout_encode',\n",
              " 'suggest',\n",
              " 'summary',\n",
              " 'sys',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'unicode_literals',\n",
              " 'util',\n",
              " 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "dir(wikipedia)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "TgY2FkTdmhTH"
      },
      "outputs": [],
      "source": [
        "#wikipedia.search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Kw5H5jMlmmS3"
      },
      "outputs": [],
      "source": [
        "page_titles = [\"Artificial Intelligence\", \"UCLAa\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ZF3BiZyXltYO"
      },
      "outputs": [],
      "source": [
        "page_title = page_titles[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Ef7yfa2jl0iZ"
      },
      "outputs": [],
      "source": [
        "search_results = wikipedia.search(page_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "YDv-CoH0Nroz"
      },
      "outputs": [],
      "source": [
        "page = wikipedia.page(search_results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "D_oiUzPaNroz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "W0IeqZZoNroz"
      },
      "outputs": [],
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "    search_results = wikipedia.search(page_title)\n",
        "    page = wikipedia.page(search_results[0])\n",
        "    return(page.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content =  get_wikipedia_content(page_title)"
      ],
      "metadata": {
        "id": "4gwSt-Lnn3Mw"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9aruncMmubX"
      },
      "source": [
        "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completions = client.chat.completions.create(\n",
        "    model = \"gpt-4\",\n",
        "    messages = [\n",
        "        {\"role\": \"system\" , \"content\" : \"I will be giving you an article. I am looking for false information. I want to capture all potentially false information, if there is even small potential for it to be wrong, please return it. Please concisely list only the false informaiton found. \"},\n",
        "         {\"role\": \"user\" , \"content\" : content[:8180]}]\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "nvtDW63gOCA2"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completions"
      ],
      "metadata": {
        "id": "l0mXVvDFPZSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9605b31e-d4bc-431f-b6e3-b4d897bc1eae"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-8vfRLvFUIeQkaiNVfOavDb57GNqqe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1. \"Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.\" - It\\'s arguably false because it could be seen as a simplification. While Alan Turing was a pivotal figure in the field of artificial intelligence, it would not be accurate to attribute the entirety of substantial early research to him alone. \\n2. \"Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture.\" - The claim oversimplifies the broader field of AI and indirectly suggests that no other techniques were impactful or relevant after 2012 and 2017, which is false.\\n3. \"The AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.\" - This could potentially be perceived as false due to the implication that significant advances in AI is overwhelmingly based in the United States, while it could be argued that AI advancements happen globally.\\n4. \"AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\" - It could be false, as not all AI applications call upon those specific fields, such as simpler rule-based systems. AI is a broad field with many applications requiring different specialties.\\n5. \"Even humans rarely use the step-by-step deduction that early AI research could model.\" - This statement could be argued as false since human reasoning often employs step-by-step logical deductions.\\n6. \"In classical planning, the agent knows exactly what the effect of any action will be.\" - The claim could be perceived as false because, in many cases, agents cannot predict with absolute certainty the result of actions due to the complexity of many decision-making situations.\\n7. \"An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.\" - This statement is arguably false because an ontology actually refers to a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.', role='assistant', function_call=None, tool_calls=None))], created=1708754683, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=419, prompt_tokens=1561, total_tokens=1980))"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "1jI-un5PnDjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "efafaf01-e9b3-4cc6-eb5d-120c63cb4800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. \"Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.\" - It's arguably false because it could be seen as a simplification. While Alan Turing was a pivotal figure in the field of artificial intelligence, it would not be accurate to attribute the entirety of substantial early research to him alone. \n",
            "2. \"Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture.\" - The claim oversimplifies the broader field of AI and indirectly suggests that no other techniques were impactful or relevant after 2012 and 2017, which is false.\n",
            "3. \"The AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.\" - This could potentially be perceived as false due to the implication that significant advances in AI is overwhelmingly based in the United States, while it could be argued that AI advancements happen globally.\n",
            "4. \"AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\" - It could be false, as not all AI applications call upon those specific fields, such as simpler rule-based systems. AI is a broad field with many applications requiring different specialties.\n",
            "5. \"Even humans rarely use the step-by-step deduction that early AI research could model.\" - This statement could be argued as false since human reasoning often employs step-by-step logical deductions.\n",
            "6. \"In classical planning, the agent knows exactly what the effect of any action will be.\" - The claim could be perceived as false because, in many cases, agents cannot predict with absolute certainty the result of actions due to the complexity of many decision-making situations.\n",
            "7. \"An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.\" - This statement is arguably false because an ontology actually refers to a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.\n"
          ]
        }
      ],
      "source": [
        "print(chat_completions.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "_TMKFGN4nDJ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e9e63549-32f7-422d-de17-5c613e47c3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. \"Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.\" - Although Alan Turing made significant contributions to the field, it's incorrect to claim that he was the first or the only one to conduct substantial research in the early stages of AI.\n",
            "2. \"Artificial intelligence was founded as an academic discipline in 1956.\" - While the term \"artificial intelligence\" was coined in 1956, research and development in the field had been going on for some time prior.\n",
            "3. \"Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques\" - Deep learning has had a major impact, but it hasn't necessarily \"surpassed all previous AI techniques\" in all areas or applications.\n",
            "4. \"Even humans rarely use the step-by-step deduction that early AI research could model.\" - Claiming how humans solve problems can vary and it's oversimplifying to say that humans rarely use step-by-step deduction.\n",
            "5. \"A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way, and a reward function that supplies the utility of each state and the cost of each action.\" - A Markov decision process doesn't necessarily include the utility of each state or the cost of each action as part of the reward function.\n",
            "6. \"The agent learns to choose actions based on what it's been rewarded for in the past.\" - This sentence implies reinforcement learning is strictly based on past rewards, which oversimplifies the process. It may also be guided by future rewards.\n"
          ]
        }
      ],
      "source": [
        "def chatgpt_error_correction(text):\n",
        "  chat_completions = client.chat.completions.create(\n",
        "    model = \"gpt-4\",\n",
        "    messages = [\n",
        "        {\"role\": \"system\" , \"content\" : \"I will be giving you an article. I am looking for false information. I want to capture all potentially false information, if there is even small potential for it to be wrong, please return it. Please concisely list only the false informaiton found. \"},\n",
        "         {\"role\": \"user\" , \"content\" : text[:8180]}]\n",
        ")\n",
        "print(chat_completions.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FKAJVXSoayA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPw5LyPEobmk"
      },
      "source": [
        "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "V7cuhML2ocGn"
      },
      "outputs": [],
      "source": [
        "page_titles = [\"snowball\",\"EFund\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "0N3XsWDWNro0"
      },
      "outputs": [],
      "source": [
        "def split_text(text, chunk_size = 8180):\n",
        "    chunk = len(text)//8180 +1\n",
        "    return([text[i*chunk_size:(i+1)*chunk_size] for i in range(0,chunks-1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "aZmEUP5sNro0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "18aafe32-8800-42d4-d864-befe47092284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_______________snowball\n",
            "_______________EFund\n",
            "ERROR\n"
          ]
        }
      ],
      "source": [
        "for page_title in page_titles:\n",
        "\n",
        "    try:\n",
        "      print(\"_______________\" + page_title)\n",
        "      content = get_wikipedia_content(page_title)\n",
        "      chatgpt_error_correction(content)\n",
        "    except:\n",
        "      print(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1keIHA9JNro0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ0x-hwwNro0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB8wEQ3WNro0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKE0iGUSNro0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEobVdIzNro0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}